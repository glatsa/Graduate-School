{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirm Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/gml/Desktop'"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data from CSV into a Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Label_Sentiment                 Label_Reason   Label_Airline  \\\n",
      "0        positive                          NaN          United   \n",
      "1        negative             Cancelled Flight  Virgin America   \n",
      "2         neutral                          NaN      US Airways   \n",
      "3        negative  Flight Attendant Complaints      US Airways   \n",
      "4        negative  Flight Attendant Complaints          United   \n",
      "\n",
      "                                                Text  \n",
      "0  @united look at this beauty ðŸ˜‰ dc-10 united air...  \n",
      "1  @VirginAmerica why Cancelled flight flight VX4...  \n",
      "2  @USAirways  I'm on flight 623 from DIA to Onta...  \n",
      "3  @USAirways  paid to upgrade to first class, we...  \n",
      "4  @united Arriving at the airport 2 hours before...  \n"
     ]
    }
   ],
   "source": [
    "data = pd.read_excel('hw4data.xlsx', header = 0)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Label_Reason']= data['Label_Reason'].replace(np.nan, 'No reason (Positive\\\\Nuetral Tweet)')\n",
    "\n",
    "issues1 = []\n",
    "fixes1 = []\n",
    "\n",
    "issues1.append('\\#[0-9]+')\n",
    "issues1.append('http\\S*')\n",
    "issues1.append('flight[s]?\\s?\\d+')\n",
    "issues1.append('[A-Z][0-9]+')\n",
    "issues1.append('\\.\\s')\n",
    "\n",
    "fixes1.append('')\n",
    "fixes1.append('')\n",
    "fixes1.append('flight')\n",
    "fixes1.append('')\n",
    "fixes1.append('')\n",
    "\n",
    "issues2.append('&amp;')\n",
    "issues2.append('-')\n",
    "issues2.append(',')\n",
    "issues2.append(\"'\")\n",
    "issues2.append('&gt;')\n",
    "\n",
    "fixes2.append('')\n",
    "fixes2.append('')\n",
    "fixes2.append('')\n",
    "fixes2.append('')\n",
    "fixes2.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    @united look at this beauty ðŸ˜‰ dc10 united airl...\n",
      "1    @VirginAmerica why Cancelled flight flight V? ...\n",
      "2    @USAirways  Im on flight from DIA to Ontario t...\n",
      "3    @USAirways  paid to upgrade to first class wen...\n",
      "4    @united Arriving at the airport 2 hours before...\n",
      "Name: Text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "cleandata = pd.Series(data.iloc[:,3])\n",
    "count = 0\n",
    "while (count < len(issues1)):\n",
    "    #print(issues[count])\n",
    "    cleandata = cleandata.str.replace(issues1[count], fixes1[count], regex=True)\n",
    "    count = count + 1\n",
    "count = 0\n",
    "while (count < len(issues2)):\n",
    "    #print(issues[count])\n",
    "    cleandata = cleandata.str.replace(issues2[count], fixes2[count], regex=False)\n",
    "    count = count + 1\n",
    "print(cleandata.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperate out Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1=data[\"Label_Sentiment\"].values\n",
    "y2=data[\"Label_Reason\"].values\n",
    "y3=data[\"Label_Airline\"].values\n",
    "x=cleandata.values\n",
    "RowNames = \" tweet#: \" + data.index.map(str) + '(' + data[\"Label_Sentiment\"] +')'\n",
    "\n",
    "x1_train, x1_test, y1_train, y1_test, idx1_train, idx1_test = train_test_split(x, y1, RowNames, test_size=0.35, random_state=0)\n",
    "x2_train, x2_test, y2_train, y2_test, idx2_train, idx2_test = train_test_split(x, y2, RowNames, test_size=0.35, random_state=0)\n",
    "x3_train, x3_test, y3_train, y3_test, idx3_train, idx3_test = train_test_split(x, y3, RowNames, test_size=0.35, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Overall Document Matrix From a Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english', ngram_range=(1,3),max_features=500,token_pattern=r'[^\\s]+')\n",
    "X = vectorizer.fit_transform(x)\n",
    "ColumnNames=vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Label_Sentiment                        Label_Reason  \\\n",
      " tweet#: 0(positive)           positive  No reason (Positive\\Nuetral Tweet)   \n",
      " tweet#: 1(negative)           negative                    Cancelled Flight   \n",
      " tweet#: 2(neutral)             neutral  No reason (Positive\\Nuetral Tweet)   \n",
      " tweet#: 3(negative)           negative         Flight Attendant Complaints   \n",
      " tweet#: 4(negative)           negative         Flight Attendant Complaints   \n",
      "...                                 ...                                 ...   \n",
      " tweet#: 2830(neutral)          neutral  No reason (Positive\\Nuetral Tweet)   \n",
      " tweet#: 2831(positive)        positive  No reason (Positive\\Nuetral Tweet)   \n",
      " tweet#: 2832(negative)        negative                         Late Flight   \n",
      " tweet#: 2833(positive)        positive  No reason (Positive\\Nuetral Tweet)   \n",
      " tweet#: 2834(negative)        negative              Customer Service Issue   \n",
      "\n",
      "                          Label_Airline  #  #fail  #united  #unitedairlines  \\\n",
      " tweet#: 0(positive)             United  0      0        0                0   \n",
      " tweet#: 1(negative)     Virgin America  0      0        0                0   \n",
      " tweet#: 2(neutral)          US Airways  0      0        0                0   \n",
      " tweet#: 3(negative)         US Airways  0      0        0                0   \n",
      " tweet#: 4(negative)             United  0      0        0                0   \n",
      "...                                 ... ..    ...      ...              ...   \n",
      " tweet#: 2830(neutral)        Southwest  0      0        0                0   \n",
      " tweet#: 2831(positive)  Virgin America  0      0        0                0   \n",
      " tweet#: 2832(negative)         JetBlue  0      0        0                0   \n",
      " tweet#: 2833(positive)  Virgin America  0      0        0                0   \n",
      " tweet#: 2834(negative)          United  0      0        0                0   \n",
      "\n",
      "                         1  1.5  10  ...  yall  year  years  yes  yesterday  \\\n",
      " tweet#: 0(positive)     0    0   0  ...     0     0      0    0          0   \n",
      " tweet#: 1(negative)     0    0   0  ...     0     0      0    0          0   \n",
      " tweet#: 2(neutral)      0    0   0  ...     0     0      0    0          0   \n",
      " tweet#: 3(negative)     0    0   0  ...     0     0      0    0          0   \n",
      " tweet#: 4(negative)     0    0   0  ...     0     0      0    0          0   \n",
      "...                     ..  ...  ..  ...   ...   ...    ...  ...        ...   \n",
      " tweet#: 2830(neutral)   0    0   0  ...     0     0      0    0          0   \n",
      " tweet#: 2831(positive)  0    0   0  ...     0     0      0    0          0   \n",
      " tweet#: 2832(negative)  0    0   0  ...     0     0      0    0          0   \n",
      " tweet#: 2833(positive)  0    0   0  ...     0     0      0    0          0   \n",
      " tweet#: 2834(negative)  0    0   0  ...     0     0      0    0          0   \n",
      "\n",
      "                         you!  you.  youre  youve  yr  \n",
      " tweet#: 0(positive)        0     0      0      0   0  \n",
      " tweet#: 1(negative)        0     0      0      0   0  \n",
      " tweet#: 2(neutral)         0     0      0      0   0  \n",
      " tweet#: 3(negative)        0     0      0      0   0  \n",
      " tweet#: 4(negative)        0     0      0      0   0  \n",
      "...                       ...   ...    ...    ...  ..  \n",
      " tweet#: 2830(neutral)      0     0      0      0   0  \n",
      " tweet#: 2831(positive)     0     0      0      0   0  \n",
      " tweet#: 2832(negative)     0     0      0      0   0  \n",
      " tweet#: 2833(positive)     0     0      0      0   0  \n",
      " tweet#: 2834(negative)     0     0      0      0   0  \n",
      "\n",
      "[2835 rows x 503 columns]\n"
     ]
    }
   ],
   "source": [
    "TDM=pd.DataFrame(X.toarray(),columns=ColumnNames, index = RowNames)\n",
    "TDM.insert(0, \"Label_Sentiment\", list(data[\"Label_Sentiment\"]), True) \n",
    "TDM.insert(1, \"Label_Reason\", list(data[\"Label_Reason\"]), True) \n",
    "TDM.insert(2, \"Label_Airline\", list(data[\"Label_Airline\"]), True) \n",
    "print(TDM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Test and Train TDMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.transform(x1_train)\n",
    "TDM1=pd.DataFrame(X_train.toarray(), columns=ColumnNames, index = idx1_train)\n",
    "TDM1.insert(0, \"Label_Sentiment\", list(y1_train), True) \n",
    "TDM1.insert(1, \"Label_Reason\", list(y2_train), True) \n",
    "TDM1.insert(2, \"Label_Airline\", list(y3_train), True) \n",
    "#print(TDM1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(x1_test)\n",
    "TDM2=pd.DataFrame(X_test.toarray(),columns=ColumnNames, index = idx1_test)\n",
    "TDM2.insert(0, \"Label_Sentiment\", list(y1_test), True) \n",
    "TDM2.insert(1, \"Label_Reason\", list(y2_test), True) \n",
    "TDM2.insert(2, \"Label_Airline\", list(y3_test), True) \n",
    "#print(TDM2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Modeling Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_clf= MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worst words for Sentiment Clustinering\n",
      "[(-5.003817232050475, 'hold'), (-5.003817232050475, 'hour'), (-4.984769037079779, 'flights'), (-4.966076904067627, 'late'), (-4.966076904067627, 'plane'), (-4.912009682797351, 'delayed'), (-4.894617940085483, '2'), (-4.877523506726182, '@jetblue'), (-4.877523506726182, 'time'), (-4.860716388409801, 'hours'), (-4.79617786727223, 'im'), (-4.750368331240936, 'customer'), (-4.664601509483511, 'service'), (-4.6509958574277315, 'just'), (-4.2365620793368075, '@southwestair'), (-4.2365620793368075, 'cancelled'), (-4.142876595259485, '@virginamerica'), (-3.8613107498251393, '@usairways'), (-3.244633933257032, 'flight'), (-2.386983039219926, '@united')]\n",
      "\n",
      "\n",
      "Worst words for Reason Clustinering\n",
      "[(-5.434813111136574, 'time'), (-5.434813111136574, 'use'), (-5.434813111136574, 'worst'), (-5.211669559822364, 'airline'), (-5.211669559822364, 'flights'), (-5.211669559822364, 'isnt'), (-5.211669559822364, 'let'), (-5.211669559822364, 'like'), (-5.211669559822364, 'seats'), (-5.211669559822364, 'ua'), (-5.02934800302841, '@jetblue'), (-5.02934800302841, '@usairways'), (-4.875197323201151, 'dont'), (-4.875197323201151, 'seat'), (-4.741665930576628, 'u'), (-4.623882894920245, 'plane'), (-4.518522379262419, 'wifi'), (-4.336200822468465, '@virginamerica'), (-3.685613256327315, 'flight'), (-2.83212342569219, '@united')]\n",
      "\n",
      "\n",
      "Top words for Airline Clustinering\n",
      "[(-6.975413927455952, '#fail'), (-6.975413927455952, '#united'), (-6.975413927455952, '#unitedairlines'), (-6.975413927455952, '1'), (-6.975413927455952, '1.5'), (-6.975413927455952, '10'), (-6.975413927455952, '15'), (-6.975413927455952, '1st class'), (-6.975413927455952, '2 hours'), (-6.975413927455952, '24'), (-6.975413927455952, '3 hours'), (-6.975413927455952, '30'), (-6.975413927455952, '45'), (-6.975413927455952, '6'), (-6.975413927455952, '7'), (-6.975413927455952, '9'), (-6.975413927455952, '@'), (-6.975413927455952, '@carrieunderwood'), (-6.975413927455952, '@delta'), (-6.975413927455952, '@ladygaga')]\n"
     ]
    }
   ],
   "source": [
    "print('Worst words for Sentiment Clustinering')\n",
    "sentiment_model = nb_clf.fit(X_train,y1_train)\n",
    "feature_ranks_sentiment = sorted(zip(sentiment_model.feature_log_prob_[0], ColumnNames))\n",
    "vn_sentiment_features = feature_ranks_sentiment[-20:]\n",
    "print(vn_sentiment_features)\n",
    "\n",
    "print('\\n')\n",
    "print('Worst words for Reason Clustinering')\n",
    "reason_model = nb_clf.fit(X_train,y2_train)\n",
    "feature_ranks_reason = sorted(zip(reason_model.feature_log_prob_[0], ColumnNames))\n",
    "vn_reason_features = feature_ranks_reason[-20:]\n",
    "print(vn_reason_features)\n",
    "\n",
    "print('\\n')\n",
    "print('Top words for Airline Clustinering')\n",
    "airline_model = nb_clf.fit(X_train,y3_train)\n",
    "feature_ranks_reason = sorted(zip(reason_model.feature_log_prob_[0], ColumnNames))\n",
    "vn_reason_features = feature_ranks_reason[:20]\n",
    "print(vn_reason_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7009063444108762\n",
      "[[ 90  34  44]\n",
      " [ 19  84 107]\n",
      " [ 35  58 522]]\n"
     ]
    }
   ],
   "source": [
    "sentiment_model = nb_clf.fit(X_train,y1_train)\n",
    "print(sentiment_model.score(X_test,y1_test))\n",
    "y1_pred = sentiment_model.fit(X_train, y1_train).predict(X_test)\n",
    "cm=confusion_matrix(y1_test, y1_pred, labels=['positive','neutral','negative'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5287009063444109\n",
      "[[ 95   4   3  49   1   5  14  13   0   3   0]\n",
      " [  8  25   0  20   0   1   4   3   0   1   0]\n",
      " [  6   1   5   9   1   1   4   4   0   0   0]\n",
      " [ 36   4   5 286   5   5  17  12   0   8   0]\n",
      " [  7   1   1  23   2   0   7   3   0   2   0]\n",
      " [  0   0   1   9   0  37   4   2   0   2   0]\n",
      " [ 14   3   3  18   2   2  56   4   0   0   0]\n",
      " [  7   1   0  46   1   0   6  11   0   0   0]\n",
      " [  3   1   2   0   0   3   2   0   0   0   0]\n",
      " [ 17   0   1  11   1   2   3   2   0   8   0]\n",
      " [  0   1   0   2   0   0   0   1   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "reason_model = nb_clf.fit(X_train,y2_train)\n",
    "print(reason_model.score(X_test,y2_test))\n",
    "y2_pred = reason_model.fit(X_train, y2_train).predict(X_test)\n",
    "cm=confusion_matrix(y2_test, y2_pred, labels=list(dict.fromkeys(y2_train)))\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9798590130916415\n",
      "[[522   1   2   0   2]\n",
      " [  1 128   0   0   2]\n",
      " [  4   0 162   0   0]\n",
      " [  1   1   1  67   1]\n",
      " [  3   0   1   0  94]]\n"
     ]
    }
   ],
   "source": [
    "airline_model = nb_clf.fit(X_train,y3_train)\n",
    "print(airline_model.score(X_test,y3_test))\n",
    "y3_pred = airline_model.fit(X_train, y3_train).predict(X_test)\n",
    "cm=confusion_matrix(y3_test, y3_pred, labels=list(dict.fromkeys(y3_train)))\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes with Cross Vailidation and Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.713217844042745\n",
      "0.5546342095024397\n",
      "0.9901193685858216\n"
     ]
    }
   ],
   "source": [
    "sentiment_model_pipe = Pipeline([('vect', CountVectorizer(stop_words='english', ngram_range=(1,3),max_features=250)),('nb', MultinomialNB())])\n",
    "scores = cross_val_score(sentiment_model_pipe, x, y1, cv=10)\n",
    "avg=sum(scores)/len(scores)\n",
    "print(avg)\n",
    "\n",
    "reason_model_pipe = Pipeline([('vect', CountVectorizer(stop_words='english', ngram_range=(1,3),max_features=250)),('nb', MultinomialNB())])\n",
    "scores = cross_val_score(reason_model_pipe, x, y2, cv=10)\n",
    "avg=sum(scores)/len(scores)\n",
    "print(avg)\n",
    "\n",
    "airline_model_pipe = Pipeline([('vect', CountVectorizer(stop_words='english', ngram_range=(1,3),max_features=250)),('nb', MultinomialNB())])\n",
    "scores = cross_val_score(airline_model_pipe, x, y3, cv=10)\n",
    "avg=sum(scores)/len(scores)\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second time without Twitter Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uses regual Expressions to find extra texts to remove for later\n",
    "issues3 = []\n",
    "fixes3 = []\n",
    "\n",
    "issues3.append('\\@\\w*')\n",
    "\n",
    "fixes3.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     look at this beauty ðŸ˜‰ dc10 united airlines ðŸ˜‰ ...\n",
      "1     why Cancelled flight flight V? one sec its de...\n",
      "2      Im on flight from DIA to Ontario tomorrow mo...\n",
      "3      paid to upgrade to first class went up to ad...\n",
      "4     Arriving at the airport 2 hours before depart...\n",
      "Name: Text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "while (count < len(issues3)):\n",
    "    #print(issues[count])\n",
    "    cleandata = cleandata.str.replace(issues3[count], fixes3[count], regex=True)\n",
    "    count = count + 1\n",
    "print(cleandata.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperate out Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1=data[\"Label_Sentiment\"].values\n",
    "y2=data[\"Label_Reason\"].values\n",
    "y3=data[\"Label_Airline\"].values\n",
    "x=cleandata.values\n",
    "RowNames = \" tweet#: \" + data.index.map(str) + '(' + data[\"Label_Sentiment\"] +')'\n",
    "\n",
    "\n",
    "x1_train, x1_test, y1_train, y1_test, idx1_train, idx1_test = train_test_split(x, y1, RowNames, test_size=0.35, random_state=0)\n",
    "x2_train, x2_test, y2_train, y2_test, idx2_train, idx2_test = train_test_split(x, y2, RowNames, test_size=0.35, random_state=0)\n",
    "x3_train, x3_test, y3_train, y3_test, idx3_train, idx3_test = train_test_split(x, y3, RowNames, test_size=0.35, random_state=0)\n",
    "\n",
    "#print(x3_train[-5:])\n",
    "#print(idx3_train[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Overall Document Matrix From a Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Label_Sentiment                        Label_Reason  \\\n",
      " tweet#: 0(positive)        positive  No reason (Positive\\Nuetral Tweet)   \n",
      " tweet#: 1(negative)        negative                    Cancelled Flight   \n",
      " tweet#: 2(neutral)          neutral  No reason (Positive\\Nuetral Tweet)   \n",
      " tweet#: 3(negative)        negative         Flight Attendant Complaints   \n",
      " tweet#: 4(negative)        negative         Flight Attendant Complaints   \n",
      "\n",
      "                       Label_Airline  !  #  #fail  #united  #unitedairlines  \\\n",
      " tweet#: 0(positive)          United  0  0      0        0                0   \n",
      " tweet#: 1(negative)  Virgin America  0  0      0        0                0   \n",
      " tweet#: 2(neutral)       US Airways  0  0      0        0                0   \n",
      " tweet#: 3(negative)      US Airways  0  0      0        0                0   \n",
      " tweet#: 4(negative)          United  0  0      0        0                0   \n",
      "\n",
      "                      1  1.5  ...  yall  year  years  yes  yesterday  you!  \\\n",
      " tweet#: 0(positive)  0    0  ...     0     0      0    0          0     0   \n",
      " tweet#: 1(negative)  0    0  ...     0     0      0    0          0     0   \n",
      " tweet#: 2(neutral)   0    0  ...     0     0      0    0          0     0   \n",
      " tweet#: 3(negative)  0    0  ...     0     0      0    0          0     0   \n",
      " tweet#: 4(negative)  0    0  ...     0     0      0    0          0     0   \n",
      "\n",
      "                      you.  youre  youve  yr  \n",
      " tweet#: 0(positive)     0      0      0   0  \n",
      " tweet#: 1(negative)     0      0      0   0  \n",
      " tweet#: 2(neutral)      0      0      0   0  \n",
      " tweet#: 3(negative)     0      0      0   0  \n",
      " tweet#: 4(negative)     0      0      0   0  \n",
      "\n",
      "[5 rows x 503 columns]\n"
     ]
    }
   ],
   "source": [
    "X = vectorizer.fit_transform(x)\n",
    "ColumnNames=vectorizer.get_feature_names()\n",
    "TDM=pd.DataFrame(X.toarray(),columns=ColumnNames, index = RowNames)\n",
    "TDM.insert(0, \"Label_Sentiment\", list(data[\"Label_Sentiment\"]), True) \n",
    "TDM.insert(1, \"Label_Reason\", list(data[\"Label_Reason\"]), True) \n",
    "TDM.insert(2, \"Label_Airline\", list(data[\"Label_Airline\"]), True) \n",
    "print(TDM.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Test and Train TDMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.transform(x1_train)\n",
    "TDM1=pd.DataFrame(X_train.toarray(), columns=ColumnNames, index = idx1_train)\n",
    "TDM1.insert(0, \"Label_Sentiment\", list(y1_train), True) \n",
    "TDM1.insert(1, \"Label_Reason\", list(y2_train), True) \n",
    "TDM1.insert(2, \"Label_Airline\", list(y3_train), True) \n",
    "#print(TDM1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(x1_test)\n",
    "TDM2=pd.DataFrame(X_test.toarray(),columns=ColumnNames, index = idx1_test)\n",
    "TDM2.insert(0, \"Label_Sentiment\", list(y1_test), True) \n",
    "TDM2.insert(1, \"Label_Reason\", list(y2_test), True) \n",
    "TDM2.insert(2, \"Label_Airline\", list(y3_test), True) \n",
    "#print(TDM1.iloc[:,36].)\n",
    "#34:73"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Modeling Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_clf= MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worst words for Sentiment Clustinering\n",
      "[(-4.957815281854906, 'like'), (-4.936309076633942, 'help'), (-4.91525566743611, 'cancelled flightled'), (-4.894636380233374, 'flightled'), (-4.874433672915854, 'dont'), (-4.835212959762574, 'hold'), (-4.835212959762574, 'hour'), (-4.816164764791878, 'flights'), (-4.797472631779726, 'late'), (-4.797472631779726, 'plane'), (-4.7434054105094505, 'delayed'), (-4.726013667797582, '2'), (-4.708919234438281, 'time'), (-4.6921121161219, 'hours'), (-4.627573594984329, 'im'), (-4.581764058953035, 'customer'), (-4.49599723719561, 'service'), (-4.482391585139831, 'just'), (-4.067957807048907, 'cancelled'), (-3.076029660969131, 'flight')]\n",
      "\n",
      "\n",
      "Worst words for Reason Clustinering\n",
      "[(-5.351858133476067, 'night'), (-5.351858133476067, 'paid'), (-5.351858133476067, 'passengers'), (-5.351858133476067, 'really'), (-5.351858133476067, 'time'), (-5.351858133476067, 'use'), (-5.351858133476067, 'worst'), (-5.128714582161857, 'airline'), (-5.128714582161857, 'flights'), (-5.128714582161857, 'isnt'), (-5.128714582161857, 'let'), (-5.128714582161857, 'like'), (-5.128714582161857, 'seats'), (-5.128714582161857, 'ua'), (-4.792242345540644, 'dont'), (-4.792242345540644, 'seat'), (-4.658710952916122, 'u'), (-4.540927917259738, 'plane'), (-4.4355674016019115, 'wifi'), (-3.6026582786668078, 'flight')]\n",
      "\n",
      "\n",
      "Top words for Airline Clustinering\n",
      "[(-6.844815479208263, '#fail'), (-6.844815479208263, '#united'), (-6.844815479208263, '#unitedairlines'), (-6.844815479208263, '1'), (-6.844815479208263, '1.5'), (-6.844815479208263, '10'), (-6.844815479208263, '15'), (-6.844815479208263, '1st class'), (-6.844815479208263, '2 hours'), (-6.844815479208263, '24'), (-6.844815479208263, '3 hours'), (-6.844815479208263, '30'), (-6.844815479208263, '45'), (-6.844815479208263, '50'), (-6.844815479208263, '6'), (-6.844815479208263, '7'), (-6.844815479208263, '9'), (-6.844815479208263, ':'), (-6.844815479208263, 'add'), (-6.844815479208263, 'adding')]\n"
     ]
    }
   ],
   "source": [
    "print('Worst words for Sentiment Clustinering')\n",
    "sentiment_model = nb_clf.fit(X_train,y1_train)\n",
    "feature_ranks_sentiment = sorted(zip(sentiment_model.feature_log_prob_[0], ColumnNames))\n",
    "vn_sentiment_features = feature_ranks_sentiment[-20:]\n",
    "print(vn_sentiment_features)\n",
    "\n",
    "print('\\n')\n",
    "print('Worst words for Reason Clustinering')\n",
    "reason_model = nb_clf.fit(X_train,y2_train)\n",
    "feature_ranks_reason = sorted(zip(reason_model.feature_log_prob_[0], ColumnNames))\n",
    "vn_reason_features = feature_ranks_reason[-20:]\n",
    "print(vn_reason_features)\n",
    "\n",
    "print('\\n')\n",
    "print('Top words for Airline Clustinering')\n",
    "airline_model = nb_clf.fit(X_train,y3_train)\n",
    "feature_ranks_reason = sorted(zip(airline_model.feature_log_prob_[0], ColumnNames))\n",
    "vn_reason_features = feature_ranks_reason[:20]\n",
    "print(vn_reason_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6868076535750252\n",
      "[[ 83  18  67]\n",
      " [ 12  71 127]\n",
      " [ 36  51 528]]\n",
      "positive neutral negative\n"
     ]
    }
   ],
   "source": [
    "sentiment_model = nb_clf.fit(X_train,y1_train)\n",
    "print(sentiment_model.score(X_test,y1_test))\n",
    "y1_pred = sentiment_model.fit(X_train, y1_train).predict(X_test)\n",
    "cm=confusion_matrix(y1_test, y1_pred, labels=['positive','neutral','negative'])\n",
    "print(cm)\n",
    "print('positive','neutral','negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5347432024169184\n",
      "[[ 95   3   4  48   2   7  12  13   0   3   0]\n",
      " [  7  26   0  20   0   1   5   1   0   1   1]\n",
      " [  7   1   6  12   1   1   3   0   0   0   0]\n",
      " [ 40   4   5 293   5   5  13   8   0   5   0]\n",
      " [  6   2   1  23   4   0   6   3   0   1   0]\n",
      " [  1   0   1  10   0  37   4   1   0   1   0]\n",
      " [ 12   3   3  22   2   2  55   2   0   1   0]\n",
      " [  5   1   0  47   3   0   7   8   0   1   0]\n",
      " [  3   1   2   0   0   3   2   0   0   0   0]\n",
      " [ 18   0   1  12   1   1   3   2   0   7   0]\n",
      " [  0   1   0   2   0   0   0   1   0   0   0]]\n",
      "['Customer Service Issue', 'Lost Luggage', 'Flight Attendant Complaints', 'No reason (Positive\\\\Nuetral Tweet)', 'Bad Flight', 'Cancelled Flight', 'Late Flight', \"Can't Tell\", 'longlines', 'Flight Booking Problems', 'Damaged Luggage']\n"
     ]
    }
   ],
   "source": [
    "reason_model = nb_clf.fit(X_train,y2_train)\n",
    "print(reason_model.score(X_test,y2_test))\n",
    "y2_pred = reason_model.fit(X_train, y2_train).predict(X_test)\n",
    "cm=confusion_matrix(y2_test, y2_pred, labels=list(dict.fromkeys(y2_train)))\n",
    "print(cm)\n",
    "print(list(dict.fromkeys(y2_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5236656596173213\n",
      "[[413  30  58   6  20]\n",
      " [ 79  31   5   4  12]\n",
      " [ 99   4  53   3   7]\n",
      " [ 46   1  14   5   5]\n",
      " [ 49  14  16   1  18]]\n",
      "['United', 'US Airways', 'Virgin America', 'JetBlue', 'Southwest']\n"
     ]
    }
   ],
   "source": [
    "airline_model = nb_clf.fit(X_train,y3_train)\n",
    "print(airline_model.score(X_test,y3_test))\n",
    "y3_pred = airline_model.fit(X_train, y3_train).predict(X_test)\n",
    "cm=confusion_matrix(y3_test, y3_pred, labels=list(dict.fromkeys(y3_train)))\n",
    "print(cm)\n",
    "print(list(dict.fromkeys(y3_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes with Cross Vailidation and Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68323345129359\n",
      "0.552893377393155\n",
      "0.544597544297776\n"
     ]
    }
   ],
   "source": [
    "sentiment_model_pipe = Pipeline([('vect', CountVectorizer(stop_words='english', ngram_range=(1,3),max_features=250)),('nb', MultinomialNB())])\n",
    "scores = cross_val_score(sentiment_model_pipe, x, y1, cv=10)\n",
    "avg=sum(scores)/len(scores)\n",
    "print(avg)\n",
    "\n",
    "reason_model_pipe = Pipeline([('vect', CountVectorizer(stop_words='english', ngram_range=(1,3),max_features=250)),('nb', MultinomialNB())])\n",
    "scores = cross_val_score(reason_model_pipe, x, y2, cv=10)\n",
    "avg=sum(scores)/len(scores)\n",
    "print(avg)\n",
    "\n",
    "airline_model_pipe = Pipeline([('vect', CountVectorizer(stop_words='english', ngram_range=(1,3),max_features=250)),('nb', MultinomialNB())])\n",
    "scores = cross_val_score(airline_model_pipe, x, y3, cv=10)\n",
    "avg=sum(scores)/len(scores)\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
