{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "from praw.models import MoreComments\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "import pprint\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import nltk\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_link_data(data, index):\n",
    "    ls = []\n",
    "    for submission in data:\n",
    "        ls.append([submission.subreddit_name_prefixed,\n",
    "                   submission.title, \n",
    "                   submission.selftext,\n",
    "                   datetime.fromtimestamp(submission.created_utc), \n",
    "                   submission.upvote_ratio, \n",
    "                   submission.score, \n",
    "                   submission.permalink,\n",
    "                   submission.comment_limit,\n",
    "                   submission.num_comments])\n",
    "    \n",
    "    df = pd.DataFrame(ls,\n",
    "                      columns=['SubReddit',\n",
    "                               'Title', \n",
    "                               'Title_Description',\n",
    "                               'Creation_Date', \n",
    "                               'Up_Vote_Ratio', \n",
    "                               'Score', \n",
    "                               'Permalink',\n",
    "                               'Comments',\n",
    "                               'Comment_Counts'],\n",
    "                      index = np.linspace(index, index + len(ls)-1, len(ls)))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## REF: https://nlpforhackers.io/topic-modeling/\n",
    "def print_topics(model, vectorizer, top_n=10):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic:  \", idx)\n",
    "      \n",
    "        print([(vectorizer.get_feature_names()[i], topic[i])\n",
    "                        for i in topic.argsort()[:-top_n - 1:-1]])\n",
    "                        ## gets top n elements in decreasing order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cid = #enter your own\n",
    "csec = #enter your own\n",
    "ua = #enter your own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "reddit = praw.Reddit(client_id= cid,\n",
    "                     client_secret= csec,\n",
    "                     user_agent= ua)\n",
    "\n",
    "print(reddit.read_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_comments_by_id': {},\n",
      " '_fetched': False,\n",
      " '_reddit': <praw.reddit.Reddit object at 0x1a19a2edd0>,\n",
      " 'all_awardings': [],\n",
      " 'allow_live_comments': False,\n",
      " 'approved_at_utc': None,\n",
      " 'approved_by': None,\n",
      " 'archived': False,\n",
      " 'author': Redditor(name='oceang1rl'),\n",
      " 'author_flair_background_color': None,\n",
      " 'author_flair_css_class': None,\n",
      " 'author_flair_richtext': [],\n",
      " 'author_flair_template_id': None,\n",
      " 'author_flair_text': None,\n",
      " 'author_flair_text_color': None,\n",
      " 'author_flair_type': 'text',\n",
      " 'author_fullname': 't2_4kqvkemc',\n",
      " 'author_patreon_flair': False,\n",
      " 'author_premium': False,\n",
      " 'awarders': [],\n",
      " 'banned_at_utc': None,\n",
      " 'banned_by': None,\n",
      " 'can_gild': False,\n",
      " 'can_mod_post': False,\n",
      " 'category': None,\n",
      " 'clicked': False,\n",
      " 'comment_limit': 2048,\n",
      " 'comment_sort': 'confidence',\n",
      " 'content_categories': None,\n",
      " 'contest_mode': False,\n",
      " 'created': 1589362472.0,\n",
      " 'created_utc': 1589333672.0,\n",
      " 'discussion_type': None,\n",
      " 'distinguished': None,\n",
      " 'domain': 'i.redd.it',\n",
      " 'downs': 0,\n",
      " 'edited': False,\n",
      " 'gilded': 0,\n",
      " 'gildings': {},\n",
      " 'hidden': False,\n",
      " 'hide_score': False,\n",
      " 'id': 'gip7cy',\n",
      " 'is_crosspostable': False,\n",
      " 'is_meta': False,\n",
      " 'is_original_content': False,\n",
      " 'is_reddit_media_domain': True,\n",
      " 'is_robot_indexable': True,\n",
      " 'is_self': False,\n",
      " 'is_video': False,\n",
      " 'likes': None,\n",
      " 'link_flair_background_color': '',\n",
      " 'link_flair_css_class': None,\n",
      " 'link_flair_richtext': [],\n",
      " 'link_flair_text': 'Photo',\n",
      " 'link_flair_text_color': 'dark',\n",
      " 'link_flair_type': 'text',\n",
      " 'locked': False,\n",
      " 'media': None,\n",
      " 'media_embed': {},\n",
      " 'media_only': False,\n",
      " 'mod_note': None,\n",
      " 'mod_reason_by': None,\n",
      " 'mod_reason_title': None,\n",
      " 'mod_reports': [],\n",
      " 'name': 't3_gip7cy',\n",
      " 'no_follow': False,\n",
      " 'num_comments': 49,\n",
      " 'num_crossposts': 1,\n",
      " 'num_reports': None,\n",
      " 'over_18': False,\n",
      " 'parent_whitelist_status': None,\n",
      " 'permalink': '/r/dogman/comments/gip7cy/here_is_the_illustrationsketch_of_what_i_saw_that/',\n",
      " 'pinned': False,\n",
      " 'post_hint': 'image',\n",
      " 'preview': {'enabled': True,\n",
      "             'images': [{'id': 'jAFoptu0ghXhhbDXWnixXwLQCzayDnaUhTwO0UMX9Rs',\n",
      "                         'resolutions': [{'height': 87,\n",
      "                                          'url': 'https://preview.redd.it/cim4dm8hrfy41.jpg?width=108&crop=smart&auto=webp&s=3b602d2049c71aade0080e996202685abb34fd85',\n",
      "                                          'width': 108},\n",
      "                                         {'height': 175,\n",
      "                                          'url': 'https://preview.redd.it/cim4dm8hrfy41.jpg?width=216&crop=smart&auto=webp&s=70bd413560d6691afef7cc84082de2cb24400782',\n",
      "                                          'width': 216},\n",
      "                                         {'height': 259,\n",
      "                                          'url': 'https://preview.redd.it/cim4dm8hrfy41.jpg?width=320&crop=smart&auto=webp&s=7c08a75c282c222fd994b2cadb2389a793481880',\n",
      "                                          'width': 320},\n",
      "                                         {'height': 518,\n",
      "                                          'url': 'https://preview.redd.it/cim4dm8hrfy41.jpg?width=640&crop=smart&auto=webp&s=d430b5d5127c55edc29dbee233afdd496c35f4de',\n",
      "                                          'width': 640},\n",
      "                                         {'height': 778,\n",
      "                                          'url': 'https://preview.redd.it/cim4dm8hrfy41.jpg?width=960&crop=smart&auto=webp&s=29b6c68af2324e26f0edad9f08d2d66cff63204f',\n",
      "                                          'width': 960}],\n",
      "                         'source': {'height': 830,\n",
      "                                    'url': 'https://preview.redd.it/cim4dm8hrfy41.jpg?auto=webp&s=dfa842bbbbb344fe9d9aaa35fe685a8a2767430b',\n",
      "                                    'width': 1024},\n",
      "                         'variants': {}}]},\n",
      " 'pwls': None,\n",
      " 'quarantine': False,\n",
      " 'removal_reason': None,\n",
      " 'removed_by': None,\n",
      " 'removed_by_category': None,\n",
      " 'report_reasons': None,\n",
      " 'saved': False,\n",
      " 'score': 184,\n",
      " 'secure_media': None,\n",
      " 'secure_media_embed': {},\n",
      " 'selftext': '',\n",
      " 'selftext_html': None,\n",
      " 'send_replies': True,\n",
      " 'spoiler': False,\n",
      " 'stickied': False,\n",
      " 'subreddit': Subreddit(display_name='dogman'),\n",
      " 'subreddit_id': 't5_3bt34',\n",
      " 'subreddit_name_prefixed': 'r/dogman',\n",
      " 'subreddit_subscribers': 3905,\n",
      " 'subreddit_type': 'public',\n",
      " 'suggested_sort': None,\n",
      " 'thumbnail': 'https://b.thumbs.redditmedia.com/R4620U8q76BNBe29iB7mJrdEtaZxf_RX2ESDjI3R-rM.jpg',\n",
      " 'thumbnail_height': 113,\n",
      " 'thumbnail_width': 140,\n",
      " 'title': '\"Here is the illustration/sketch of what I saw that night. I am a '\n",
      "          'wildlife artist with a very keen eye and photographic memory. I am '\n",
      "          'use to drawing musculature, accurate portrayals of animals so when '\n",
      "          'I explain what I saw hopefully there is a bit of credibility.\" '\n",
      "          'Lindagodfrey.com > November 7, 2016',\n",
      " 'total_awards_received': 0,\n",
      " 'treatment_tags': [],\n",
      " 'ups': 184,\n",
      " 'upvote_ratio': 0.98,\n",
      " 'url': 'https://i.redd.it/cim4dm8hrfy41.jpg',\n",
      " 'user_reports': [],\n",
      " 'view_count': None,\n",
      " 'visited': False,\n",
      " 'whitelist_status': None,\n",
      " 'wls': None}\n",
      "{'_comments_by_id': {},\n",
      " '_fetched': False,\n",
      " '_reddit': <praw.reddit.Reddit object at 0x1a19a2edd0>,\n",
      " 'all_awardings': [],\n",
      " 'allow_live_comments': False,\n",
      " 'approved_at_utc': None,\n",
      " 'approved_by': None,\n",
      " 'archived': False,\n",
      " 'author': Redditor(name='Darththorn'),\n",
      " 'author_flair_background_color': None,\n",
      " 'author_flair_css_class': None,\n",
      " 'author_flair_richtext': [],\n",
      " 'author_flair_template_id': '840f8fba-5f4a-11ea-9a47-0e2da4401669',\n",
      " 'author_flair_text': 'Believer',\n",
      " 'author_flair_text_color': 'dark',\n",
      " 'author_flair_type': 'text',\n",
      " 'author_fullname': 't2_aqkoo',\n",
      " 'author_patreon_flair': False,\n",
      " 'author_premium': True,\n",
      " 'awarders': [],\n",
      " 'banned_at_utc': None,\n",
      " 'banned_by': None,\n",
      " 'can_gild': False,\n",
      " 'can_mod_post': False,\n",
      " 'category': None,\n",
      " 'clicked': False,\n",
      " 'comment_limit': 2048,\n",
      " 'comment_sort': 'confidence',\n",
      " 'content_categories': None,\n",
      " 'contest_mode': False,\n",
      " 'created': 1583488042.0,\n",
      " 'created_utc': 1583459242.0,\n",
      " 'discussion_type': None,\n",
      " 'distinguished': 'moderator',\n",
      " 'domain': 'self.dogman',\n",
      " 'downs': 0,\n",
      " 'edited': False,\n",
      " 'gilded': 0,\n",
      " 'gildings': {},\n",
      " 'hidden': False,\n",
      " 'hide_score': False,\n",
      " 'id': 'fe642h',\n",
      " 'is_crosspostable': False,\n",
      " 'is_meta': False,\n",
      " 'is_original_content': False,\n",
      " 'is_reddit_media_domain': False,\n",
      " 'is_robot_indexable': True,\n",
      " 'is_self': True,\n",
      " 'is_video': False,\n",
      " 'likes': None,\n",
      " 'link_flair_background_color': '',\n",
      " 'link_flair_css_class': None,\n",
      " 'link_flair_richtext': [],\n",
      " 'link_flair_text': None,\n",
      " 'link_flair_text_color': 'dark',\n",
      " 'link_flair_type': 'text',\n",
      " 'locked': False,\n",
      " 'media': None,\n",
      " 'media_embed': {},\n",
      " 'media_only': False,\n",
      " 'mod_note': None,\n",
      " 'mod_reason_by': None,\n",
      " 'mod_reason_title': None,\n",
      " 'mod_reports': [],\n",
      " 'name': 't3_fe642h',\n",
      " 'no_follow': False,\n",
      " 'num_comments': 16,\n",
      " 'num_crossposts': 0,\n",
      " 'num_reports': None,\n",
      " 'over_18': False,\n",
      " 'parent_whitelist_status': None,\n",
      " 'permalink': '/r/dogman/comments/fe642h/new_mod/',\n",
      " 'pinned': False,\n",
      " 'pwls': None,\n",
      " 'quarantine': False,\n",
      " 'removal_reason': None,\n",
      " 'removed_by': None,\n",
      " 'removed_by_category': None,\n",
      " 'report_reasons': None,\n",
      " 'saved': False,\n",
      " 'score': 109,\n",
      " 'secure_media': None,\n",
      " 'secure_media_embed': {},\n",
      " 'selftext': \"Hi, I'm now the mod of this subreddit because the old mod wasn't \"\n",
      "             'active on Reddit for years.\\n'\n",
      "             '\\n'\n",
      "             \"I've gone through the mod queue and removed all the spam and \"\n",
      "             'other stuff that had been reported.\\n'\n",
      "             '\\n'\n",
      "             \"I've also set up a bot that will post all the newest episodes of \"\n",
      "             'Dog Man encounters straight to the subreddit automatically.\\n'\n",
      "             '\\n'\n",
      "             'If anyone is good with graphics or design and wants to make some '\n",
      "             'for the subreddit let me know!\\n'\n",
      "             '\\n'\n",
      "             'Cheers.',\n",
      " 'selftext_html': '<!-- SC_OFF --><div class=\"md\"><p>Hi, I&#39;m now the mod '\n",
      "                  'of this subreddit because the old mod wasn&#39;t active on '\n",
      "                  'Reddit for years.</p>\\n'\n",
      "                  '\\n'\n",
      "                  '<p>I&#39;ve gone through the mod queue and removed all the '\n",
      "                  'spam and other stuff that had been reported.</p>\\n'\n",
      "                  '\\n'\n",
      "                  '<p>I&#39;ve also set up a bot that will post all the newest '\n",
      "                  'episodes of Dog Man encounters straight to the subreddit '\n",
      "                  'automatically.</p>\\n'\n",
      "                  '\\n'\n",
      "                  '<p>If anyone is good with graphics or design and wants to '\n",
      "                  'make some for the subreddit let me know!</p>\\n'\n",
      "                  '\\n'\n",
      "                  '<p>Cheers.</p>\\n'\n",
      "                  '</div><!-- SC_ON -->',\n",
      " 'send_replies': True,\n",
      " 'spoiler': False,\n",
      " 'stickied': True,\n",
      " 'subreddit': Subreddit(display_name='dogman'),\n",
      " 'subreddit_id': 't5_3bt34',\n",
      " 'subreddit_name_prefixed': 'r/dogman',\n",
      " 'subreddit_subscribers': 3905,\n",
      " 'subreddit_type': 'public',\n",
      " 'suggested_sort': None,\n",
      " 'thumbnail': 'self',\n",
      " 'thumbnail_height': None,\n",
      " 'thumbnail_width': None,\n",
      " 'title': 'New mod.',\n",
      " 'total_awards_received': 0,\n",
      " 'treatment_tags': [],\n",
      " 'ups': 109,\n",
      " 'upvote_ratio': 1.0,\n",
      " 'url': 'https://www.reddit.com/r/dogman/comments/fe642h/new_mod/',\n",
      " 'user_reports': [],\n",
      " 'view_count': None,\n",
      " 'visited': False,\n",
      " 'whitelist_status': None,\n",
      " 'wls': None}\n"
     ]
    }
   ],
   "source": [
    "#Display only\n",
    "data = reddit.subreddit('dogman').top(limit = 2)\n",
    "for submission in data:\n",
    "    pprint.pprint(vars(submission))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of         SubReddit                                              Title  \\\n",
      "0.0     r/Cooking                     Healthy Non Fried McPizza Puff   \n",
      "1.0     r/Cooking                        Help with chimichurri steak   \n",
      "2.0     r/Cooking  Am I supposed to add salt to the water before ...   \n",
      "3.0     r/Cooking  Any other former line cooks out there that tra...   \n",
      "4.0     r/Cooking  Anyone know of an app or website that can spec...   \n",
      "...           ...                                                ...   \n",
      "75.0  r/investing  John Bogle, who founded Vanguard and revolutio...   \n",
      "76.0  r/investing  The lawyer who took on Big Tobacco and Enron i...   \n",
      "77.0  r/investing  Today's stock market crash was worse than the ...   \n",
      "78.0  r/investing  \"Brands don’t need Amazon\". Nike's decision to...   \n",
      "79.0  r/investing  70% of new home purchases in China are second ...   \n",
      "\n",
      "                                      Title_Description       Creation_Date  \\\n",
      "0.0   Easiest Pizza McPuff Recipe \\n\\nStay healthy w... 2020-05-28 01:19:18   \n",
      "1.0   So i was going to marinate my steak in some of... 2020-05-28 01:04:53   \n",
      "2.0   I know that when you cook pasta you're suppose... 2020-05-28 01:02:12   \n",
      "3.0   From 2017-2019 I was line cooking at bars, bis... 2020-05-28 00:10:51   \n",
      "4.0   Hi, folks!!\\n\\nSo I'm doing the Milk Street sc... 2020-05-28 00:10:46   \n",
      "...                                                 ...                 ...   \n",
      "75.0  http://www.philly.com/business/a/john-bogle-de... 2019-01-16 15:07:13   \n",
      "76.0  http://www.businessinsider.com/class-action-at... 2017-04-20 12:25:23   \n",
      "77.0                                                    2020-03-16 14:00:11   \n",
      "78.0  Jefferies analyst Randy Konik said “Amazon had... 2019-11-13 23:28:13   \n",
      "79.0  https://pbs.twimg.com/media/DriO-EXXQAI8pLv?fo... 2018-11-11 12:47:33   \n",
      "\n",
      "      Up_Vote_Ratio  Score                                          Permalink  \\\n",
      "0.0            0.67      1  /r/Cooking/comments/gs1j0o/healthy_non_fried_m...   \n",
      "1.0            1.00      1  /r/Cooking/comments/gs1d17/help_with_chimichur...   \n",
      "2.0            1.00      1  /r/Cooking/comments/gs1bt3/am_i_supposed_to_ad...   \n",
      "3.0            0.67      1  /r/Cooking/comments/gs0pwz/any_other_former_li...   \n",
      "4.0            1.00      1  /r/Cooking/comments/gs0pvj/anyone_know_of_an_a...   \n",
      "...             ...    ...                                                ...   \n",
      "75.0           0.95   5271  /r/investing/comments/agqhfk/john_bogle_who_fo...   \n",
      "76.0           0.94   5080  /r/investing/comments/66jmb5/the_lawyer_who_to...   \n",
      "77.0           0.98   5000  /r/investing/comments/fjrbjn/todays_stock_mark...   \n",
      "78.0           0.97   4758  /r/investing/comments/dw5gfq/brands_dont_need_...   \n",
      "79.0           0.97   4489  /r/investing/comments/9w6sqk/70_of_new_home_pu...   \n",
      "\n",
      "      Comments  Comment_Counts  \n",
      "0.0       2048               0  \n",
      "1.0       2048               0  \n",
      "2.0       2048               7  \n",
      "3.0       2048               3  \n",
      "4.0       2048               2  \n",
      "...        ...             ...  \n",
      "75.0      2048             298  \n",
      "76.0      2048             251  \n",
      "77.0      2048            1726  \n",
      "78.0      2048             566  \n",
      "79.0      2048             511  \n",
      "\n",
      "[80 rows x 9 columns]>\n"
     ]
    }
   ],
   "source": [
    "del data\n",
    "\n",
    "data = reddit.subreddit('cooking').new(limit = 20)\n",
    "df = store_link_data(data,0)\n",
    "\n",
    "data = reddit.subreddit('beer').hot(limit = 20)\n",
    "df = df.append(store_link_data(data, len(df.index)))\n",
    "    \n",
    "data = reddit.subreddit('computer').rising(limit = 20)\n",
    "df = df.append(store_link_data(data,len(df.index)))\n",
    "    \n",
    "data = reddit.subreddit('investing').top(limit = 20)\n",
    "df = df.append(store_link_data(data,len(df.index)))\n",
    "\n",
    "print(df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(df)):\n",
    "    url = \"https://www.reddit.com\" + df.iloc[x, 6]\n",
    "    submission = reddit.submission(url=url)\n",
    "    #submission.comment_sort = \"top\"\n",
    "    counting = 1\n",
    "    submission.comments.replace_more(limit=None)\n",
    "    for comment in submission.comments:\n",
    "        if isinstance(comment, MoreComments):\n",
    "            continue\n",
    "        if counting == 1:\n",
    "            comt = ''\n",
    "        comt = comt + 'Author: {}, Ups: {}, Date: {}, \\nComment: {}\\n\\n'.format(comment.author,\n",
    "                                                                      comment.ups,\n",
    "                                                                      datetime.fromtimestamp(submission.created_utc),\n",
    "                                                                      comment.body)\n",
    "        counting = counting + 1\n",
    "        \n",
    "    if counting > 1:\n",
    "        df.iloc[x,7] = comt\n",
    "        df.iloc[x,8] = counting - 1\n",
    "            \n",
    "    else:\n",
    "        df.iloc[x,7] = ''\n",
    "        df.iloc[x,8] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(df)):\n",
    "    url = \"https://www.reddit.com\" + df.iloc[x, 6]\n",
    "    submission = reddit.submission(url=url)\n",
    "    submission.comments.replace_more(limit=None)\n",
    "    ls = []\n",
    "    for comment in submission.comments:\n",
    "        if isinstance(comment, MoreComments):\n",
    "            continue\n",
    "        ls.append([comment.subreddit_name_prefixed,comment.body])\n",
    "    \n",
    "    if x is 0:\n",
    "        tiny_df = pd.DataFrame(ls, columns=['LABEL','TEXT'],\n",
    "                               index = np.linspace(0, len(ls)-1, len(ls)))\n",
    "    else:\n",
    "        df2 = pd.DataFrame(ls, columns=['LABEL', 'TEXT'], \n",
    "                           index = np.linspace(0, len(ls)-1, len(ls)))\n",
    "        tiny_df = tiny_df.append(df2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(df)):\n",
    "    df.iloc[x,2] = df.iloc[x,1] + ' ' + df.iloc[x,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models for Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gml/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel poly with Accuracy for C=0.01:0.14285714285714285\n",
      "Kernel poly with Accuracy for C=1:0.14285714285714285\n",
      "Kernel poly with Accuracy for C=10:0.14285714285714285\n",
      "Kernel poly with Accuracy for C=100:0.14285714285714285\n",
      "Kernel rbf with Accuracy for C=0.01:0.14285714285714285\n",
      "Kernel rbf with Accuracy for C=1:0.14285714285714285\n",
      "Kernel rbf with Accuracy for C=10:0.17857142857142858\n",
      "Kernel rbf with Accuracy for C=100:0.6071428571428571\n",
      "Kernel linear with Accuracy for C=0.01:0.35714285714285715\n",
      "Kernel linear with Accuracy for C=1:0.5357142857142857\n",
      "Kernel linear with Accuracy for C=10:0.5357142857142857\n",
      "Kernel linear with Accuracy for C=100:0.5357142857142857\n",
      "0.775\n"
     ]
    }
   ],
   "source": [
    "ngram_vectorizer = CountVectorizer(binary=False, stop_words = \"english\",ngram_range=(1, 2))\n",
    "ngram_vectorizer.fit(df['Title_Description'])\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['Title_Description'], df['SubReddit'], train_size = 0.65,random_state = 0)\n",
    "ColumnNames = ngram_vectorizer.get_feature_names()\n",
    "X_TRAIN = ngram_vectorizer.transform(x_train)\n",
    "TDM1=pd.DataFrame(X_TRAIN.toarray(),columns=ColumnNames)\n",
    "X_TEST = ngram_vectorizer.transform(x_test)\n",
    "TDM2=pd.DataFrame(X_TEST.toarray(),columns=ColumnNames)\n",
    "\n",
    "for a in ['poly', 'rbf','linear']:\n",
    "    for c in [0.01, 1, 10,100]: \n",
    "        svm = SVC(kernel = a,C = c, gamma = 'auto')\n",
    "        svm.fit(TDM1, y_train)\n",
    "        print (\"Kernel %s with Accuracy for C=%s:%s\" \n",
    "               % (a , c ,accuracy_score(y_test, svm.predict(TDM2))))\n",
    "\n",
    "nb_clf= MultinomialNB()\n",
    "scores = cross_val_score(nb_clf, ngram_vectorizer.transform(df['Title_Description']), df['SubReddit'], cv=10)\n",
    "avg=sum(scores)/len(scores)\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gml/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel poly with Accuracy for C=0.01:0.14285714285714285\n",
      "Kernel poly with Accuracy for C=1:0.14285714285714285\n",
      "Kernel poly with Accuracy for C=10:0.14285714285714285\n",
      "Kernel poly with Accuracy for C=100:0.14285714285714285\n",
      "Kernel rbf with Accuracy for C=0.01:0.14285714285714285\n",
      "Kernel rbf with Accuracy for C=1:0.14285714285714285\n",
      "Kernel rbf with Accuracy for C=10:0.14285714285714285\n",
      "Kernel rbf with Accuracy for C=100:0.14285714285714285\n",
      "Kernel linear with Accuracy for C=0.01:0.14285714285714285\n",
      "Kernel linear with Accuracy for C=1:0.4642857142857143\n",
      "Kernel linear with Accuracy for C=10:0.4642857142857143\n",
      "Kernel linear with Accuracy for C=100:0.4642857142857143\n",
      "0.85\n"
     ]
    }
   ],
   "source": [
    "ngram_vectorizer = TfidfVectorizer(binary=False, stop_words = \"english\",ngram_range=(1, 2))\n",
    "ngram_vectorizer.fit(df['Title_Description'])\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['Title_Description'], df['SubReddit'], train_size = 0.65,random_state = 0)\n",
    "ColumnNames = ngram_vectorizer.get_feature_names()\n",
    "X_TRAIN = ngram_vectorizer.transform(x_train)\n",
    "TDM1=pd.DataFrame(X_TRAIN.toarray(),columns=ColumnNames)\n",
    "X_TEST = ngram_vectorizer.transform(x_test)\n",
    "TDM2=pd.DataFrame(X_TEST.toarray(),columns=ColumnNames)\n",
    "\n",
    "for a in ['poly', 'rbf','linear']:\n",
    "    for c in [0.01, 1, 10,100]: \n",
    "        svm = SVC(kernel = a,C = c, gamma = 'auto')\n",
    "        svm.fit(TDM1, y_train)\n",
    "        print (\"Kernel %s with Accuracy for C=%s:%s\" \n",
    "               % (a , c ,accuracy_score(y_test, svm.predict(TDM2))))\n",
    "\n",
    "nb_clf= MultinomialNB()\n",
    "scores = cross_val_score(nb_clf, ngram_vectorizer.transform(df['Title_Description']), df['SubReddit'], cv=10)\n",
    "avg=sum(scores)/len(scores)\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gml/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel poly with Accuracy for C=0.01:0.14285714285714285\n",
      "Kernel poly with Accuracy for C=1:0.14285714285714285\n",
      "Kernel poly with Accuracy for C=10:0.14285714285714285\n",
      "Kernel poly with Accuracy for C=100:0.14285714285714285\n",
      "Kernel rbf with Accuracy for C=0.01:0.14285714285714285\n",
      "Kernel rbf with Accuracy for C=1:0.14285714285714285\n",
      "Kernel rbf with Accuracy for C=10:0.14285714285714285\n",
      "Kernel rbf with Accuracy for C=100:0.42857142857142855\n",
      "Kernel linear with Accuracy for C=0.01:0.17857142857142858\n",
      "Kernel linear with Accuracy for C=1:0.5\n",
      "Kernel linear with Accuracy for C=10:0.5\n",
      "Kernel linear with Accuracy for C=100:0.5\n",
      "0.7375\n"
     ]
    }
   ],
   "source": [
    "ngram_vectorizer = CountVectorizer(binary=True, stop_words = \"english\",ngram_range=(1, 2))\n",
    "ngram_vectorizer.fit(df['Title_Description'])\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['Title_Description'], df['SubReddit'], train_size = 0.65,random_state = 0)\n",
    "ColumnNames = ngram_vectorizer.get_feature_names()\n",
    "X_TRAIN = ngram_vectorizer.transform(x_train)\n",
    "TDM1=pd.DataFrame(X_TRAIN.toarray(),columns=ColumnNames)\n",
    "X_TEST = ngram_vectorizer.transform(x_test)\n",
    "TDM2=pd.DataFrame(X_TEST.toarray(),columns=ColumnNames)\n",
    "\n",
    "for a in ['poly', 'rbf','linear']:\n",
    "    for c in [0.01, 1, 10,100]: \n",
    "        svm = SVC(kernel = a,C = c, gamma = 'auto')\n",
    "        svm.fit(TDM1, y_train)\n",
    "        print (\"Kernel %s with Accuracy for C=%s:%s\" \n",
    "               % (a , c ,accuracy_score(y_test, svm.predict(TDM2))))\n",
    "\n",
    "nb_clf= MultinomialNB()\n",
    "scores = cross_val_score(nb_clf, ngram_vectorizer.transform(df['Title_Description']), df['SubReddit'], cv=10)\n",
    "avg=sum(scores)/len(scores)\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models for Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gml/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel poly with Accuracy for C=0.01:0.8918387413962635\n",
      "Kernel poly with Accuracy for C=1:0.8918387413962635\n",
      "Kernel poly with Accuracy for C=10:0.8918387413962635\n",
      "Kernel poly with Accuracy for C=100:0.8918387413962635\n",
      "Kernel rbf with Accuracy for C=0.01:0.8918387413962635\n",
      "Kernel rbf with Accuracy for C=1:0.9006882989183874\n",
      "Kernel rbf with Accuracy for C=10:0.9124877089478859\n",
      "Kernel rbf with Accuracy for C=100:0.9252704031465093\n",
      "Kernel linear with Accuracy for C=0.01:0.9065880039331367\n",
      "Kernel linear with Accuracy for C=1:0.9183874139626352\n",
      "Kernel linear with Accuracy for C=10:0.880039331366765\n",
      "Kernel linear with Accuracy for C=100:0.8751229105211407\n",
      "0.8969337059419253\n"
     ]
    }
   ],
   "source": [
    "ngram_vectorizer = CountVectorizer(binary=False, stop_words = \"english\",ngram_range=(1, 2), max_features = 700)\n",
    "ngram_vectorizer.fit(tiny_df['TEXT'])\n",
    "x_train, x_test, y_train, y_test = train_test_split(tiny_df['TEXT'], tiny_df['LABEL'], train_size = 0.65,random_state = 0)\n",
    "ColumnNames = ngram_vectorizer.get_feature_names()\n",
    "X_TRAIN = ngram_vectorizer.transform(x_train)\n",
    "TDM1=pd.DataFrame(X_TRAIN.toarray(),columns=ColumnNames)\n",
    "X_TEST = ngram_vectorizer.transform(x_test)\n",
    "TDM2=pd.DataFrame(X_TEST.toarray(),columns=ColumnNames)\n",
    "\n",
    "for a in ['poly', 'rbf','linear']:\n",
    "    for c in [0.01, 1, 10,100]: \n",
    "        svm = SVC(kernel = a,C = c, gamma = 'auto')\n",
    "        svm.fit(TDM1, y_train)\n",
    "        print (\"Kernel %s with Accuracy for C=%s:%s\" \n",
    "               % (a , c ,accuracy_score(y_test, svm.predict(TDM2))))\n",
    "\n",
    "nb_clf= MultinomialNB()\n",
    "scores = cross_val_score(nb_clf, ngram_vectorizer.transform(tiny_df['TEXT']), tiny_df['LABEL'], cv=10)\n",
    "avg=sum(scores)/len(scores)\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gml/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel poly with Accuracy for C=0.01:0.8918387413962635\n",
      "Kernel poly with Accuracy for C=1:0.8918387413962635\n",
      "Kernel poly with Accuracy for C=10:0.8918387413962635\n",
      "Kernel poly with Accuracy for C=100:0.8918387413962635\n",
      "Kernel rbf with Accuracy for C=0.01:0.8918387413962635\n",
      "Kernel rbf with Accuracy for C=1:0.8918387413962635\n",
      "Kernel rbf with Accuracy for C=10:0.8918387413962635\n",
      "Kernel rbf with Accuracy for C=100:0.9164208456243854\n",
      "Kernel linear with Accuracy for C=0.01:0.8918387413962635\n",
      "Kernel linear with Accuracy for C=1:0.9311701081612586\n",
      "Kernel linear with Accuracy for C=10:0.9026548672566371\n",
      "Kernel linear with Accuracy for C=100:0.8859390363815143\n",
      "0.916624331201515\n"
     ]
    }
   ],
   "source": [
    "ngram_vectorizer = TfidfVectorizer(binary=False, stop_words = \"english\",ngram_range=(1, 2), max_features = 700)\n",
    "ngram_vectorizer.fit(tiny_df['TEXT'])\n",
    "x_train, x_test, y_train, y_test = train_test_split(tiny_df['TEXT'], tiny_df['LABEL'], train_size = 0.65,random_state = 0)\n",
    "ColumnNames = ngram_vectorizer.get_feature_names()\n",
    "X_TRAIN = ngram_vectorizer.transform(x_train)\n",
    "TDM1=pd.DataFrame(X_TRAIN.toarray(),columns=ColumnNames)\n",
    "X_TEST = ngram_vectorizer.transform(x_test)\n",
    "TDM2=pd.DataFrame(X_TEST.toarray(),columns=ColumnNames)\n",
    "\n",
    "for a in ['poly', 'rbf','linear']:\n",
    "    for c in [0.01, 1, 10,100]: \n",
    "        svm = SVC(kernel = a,C = c, gamma = 'auto')\n",
    "        svm.fit(TDM1, y_train)\n",
    "        print (\"Kernel %s with Accuracy for C=%s:%s\" \n",
    "               % (a , c ,accuracy_score(y_test, svm.predict(TDM2))))\n",
    "\n",
    "nb_clf= MultinomialNB()\n",
    "scores = cross_val_score(nb_clf, ngram_vectorizer.transform(tiny_df['TEXT']), tiny_df['LABEL'], cv=10)\n",
    "avg=sum(scores)/len(scores)\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gml/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel poly with Accuracy for C=0.01:0.8918387413962635\n",
      "Kernel poly with Accuracy for C=0.05:0.8918387413962635\n",
      "Kernel poly with Accuracy for C=0.25:0.8918387413962635\n",
      "Kernel poly with Accuracy for C=0.5:0.8918387413962635\n",
      "Kernel poly with Accuracy for C=1:0.8918387413962635\n",
      "Kernel rbf with Accuracy for C=0.01:0.8918387413962635\n",
      "Kernel rbf with Accuracy for C=0.05:0.8918387413962635\n",
      "Kernel rbf with Accuracy for C=0.25:0.8918387413962635\n",
      "Kernel rbf with Accuracy for C=0.5:0.8918387413962635\n",
      "Kernel rbf with Accuracy for C=1:0.9006882989183874\n",
      "Kernel linear with Accuracy for C=0.01:0.9026548672566371\n",
      "Kernel linear with Accuracy for C=0.05:0.9203539823008849\n",
      "Kernel linear with Accuracy for C=0.25:0.9262536873156342\n",
      "Kernel linear with Accuracy for C=0.5:0.9272369714847591\n",
      "Kernel linear with Accuracy for C=1:0.9233038348082596\n",
      "0.8914352134599293\n"
     ]
    }
   ],
   "source": [
    "ngram_vectorizer = CountVectorizer(binary=True, stop_words = \"english\",ngram_range=(1, 2), max_features = 700)\n",
    "ngram_vectorizer.fit(tiny_df['TEXT'])\n",
    "x_train, x_test, y_train, y_test = train_test_split(tiny_df['TEXT'], tiny_df['LABEL'], train_size = 0.65,random_state = 0)\n",
    "ColumnNames = ngram_vectorizer.get_feature_names()\n",
    "X_TRAIN = ngram_vectorizer.transform(x_train)\n",
    "TDM1=pd.DataFrame(X_TRAIN.toarray(),columns=ColumnNames)\n",
    "X_TEST = ngram_vectorizer.transform(x_test)\n",
    "TDM2=pd.DataFrame(X_TEST.toarray(),columns=ColumnNames)\n",
    "\n",
    "for a in ['poly', 'rbf','linear']:\n",
    "    for c in [0.01, 0.05, 0.25, 0.5, 1]: \n",
    "        svm = SVC(kernel = a,C = c, gamma = 'auto')\n",
    "        svm.fit(TDM1, y_train)\n",
    "        print (\"Kernel %s with Accuracy for C=%s:%s\" \n",
    "               % (a , c ,accuracy_score(y_test, svm.predict(TDM2))))\n",
    "\n",
    "nb_clf= MultinomialNB()\n",
    "scores = cross_val_score(nb_clf, ngram_vectorizer.transform(tiny_df['TEXT']), tiny_df['LABEL'], cv=10)\n",
    "avg=sum(scores)/len(scores)\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('hw7data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_df.to_csv('hw7datap2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
